# RLM (Recursive Language Model) Project

A recursive language model system where a root LLM (boss) delegates context reading to a smaller sub-LLM (intern) iteratively until finding the optimal answer.

## Architecture

```
RLM (Boss)
â”œâ”€â”€ Formulates high-level questions
â”œâ”€â”€ Evaluates answers from SubLLM
â””â”€â”€ Iterates until satisfied

SubLLM (Intern)
â”œâ”€â”€ Reads context from REPL environment
â”œâ”€â”€ Answers specific questions
â””â”€â”€ Returns findings to RLM
```

## Features

- âœ… Recursive question-answering with configurable depth
- âœ… Boss-intern model with iterative refinement
- âœ… REPL environment with context variable
- âœ… Environment-based model configuration
- âœ… Logging system for full iteration tracking
- âœ… Pluggable test questions system

## Setup

1. Install dependencies:
```bash
pip install -r requirements.txt --break-system-packages
```

2. Configure your API key:
```bash
cp .env.example .env
# Edit .env and add your OpenAI API key
```

3. Run the REPL:
```bash
python src/repl.py
```

## Project Structure

```
rlm-project/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ rlm.py           # Main RLM (boss) logic
â”‚   â”œâ”€â”€ sublm.py         # SubLLM (intern) logic
â”‚   â”œâ”€â”€ repl.py          # Interactive REPL environment
â”‚   â”œâ”€â”€ logger.py        # Logging system
â”‚   â””â”€â”€ config.py        # Configuration loader
â”œâ”€â”€ tests/
â”‚   â””â”€â”€ test_questions.json  # Pluggable test questions
â”œâ”€â”€ config/
â”‚   â””â”€â”€ prompts.py       # System prompts
â”œâ”€â”€ logs/               # Iteration logs (auto-generated)
â”œâ”€â”€ .env               # Environment variables
â””â”€â”€ requirements.txt   # Python dependencies
```

## Usage

### Interactive REPL

```python
# Start REPL
python src/repl.py

# Set context
> context = "Your knowledge base here..."

# Ask a question
> ask("What is the main topic?")
```

### Running Test Questions

```python
from src.rlm import RLM
from tests.test_questions import load_questions

rlm = RLM(max_depth=3)
questions = load_questions()

for q in questions:
    answer = rlm.answer(q["question"], q["context"])
    print(f"Q: {q['question']}\nA: {answer}\n")
```

## Configuration

Edit `.env`:
```
OPENAI_API_KEY=your_key_here
RLM_MODEL=gpt-4  # Boss model
SUBLM_MODEL=gpt-4o-mini  # Intern model
MAX_ITERATIONS=5
MAX_DEPTH=1
ENABLE_LOGGING=true
LOG_LEVEL=INFO
```

## Extending the Project

This is designed as a foundation. Next steps:
1. âœ… Add test question system
2. âœ… Implement detailed logging
3. ðŸ”„ Add vector database support
4. ðŸ”„ Implement caching layer
5. ðŸ”„ Add metrics and evaluation

## License

MIT
